{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Reasoning\n",
        "\n",
        "## Made by: Wilfredo Aaron Sosa Ramos\n",
        "\n",
        "This notebook compares how LLMs from different Generative AI providers perform on three examples that can show issues with LLM reasoning:\n",
        "\n",
        "* [The Reversal Curse](https://github.com/lukasberglund/reversal_curse) shows that LLMs trained on \"A is B\" fail to learn \"B is A\".\n",
        "* [How many r's in the word strawberry?](https://x.com/karpathy/status/1816637781659254908) shows \"the weirdness of LLM Tokenization\".  \n",
        "* [Which number is bigger, 9.11 or 9.9?](https://x.com/DrJimFan/status/1816521330298356181) shows that \"LLMs are alien beasts.\"\n",
        "\n",
        "Resource: https://github.com/andrewyng/aisuite/blob/main/examples/llm_reasoning.ipynb"
      ],
      "metadata": {
        "id": "B7VkOKRGX3oP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MCjkT--OwTJ",
        "outputId": "77c411ad-d4c0-4cc1-a037-49d7427fd451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aisuite\n",
            "  Downloading aisuite-0.1.6-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (1.54.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.66.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.26.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.2.3)\n",
            "Downloading aisuite-0.1.6-py3-none-any.whl (20 kB)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aisuite, groq, anthropic\n",
            "Successfully installed aisuite-0.1.6 anthropic-0.30.1 groq-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install aisuite 'aisuite[all]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "JULb1Sn6Rdxy",
        "outputId": "58640f08-254a-48c1-be10-e355cfbfd8e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ],
      "metadata": {
        "id": "zWuMT5A4T8Si"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vertexai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "Oiv_-aKmU2Zl",
        "outputId": "72acb9f7-fe63-4591-8d77-4b73d9535dff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vertexai\n",
            "  Downloading vertexai-1.71.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-aiplatform==1.71.1 (from google-cloud-aiplatform[all]==1.71.1->vertexai)\n",
            "  Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (4.25.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (24.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.13.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.10.3)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.16)\n",
            "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (4.12.2)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.26.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2024.12.14)\n",
            "Downloading vertexai-1.71.1-py3-none-any.whl (7.3 kB)\n",
            "Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-aiplatform, vertexai\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 1.73.0\n",
            "    Uninstalling google-cloud-aiplatform-1.73.0:\n",
            "      Successfully uninstalled google-cloud-aiplatform-1.73.0\n",
            "Successfully installed google-cloud-aiplatform-1.71.1 vertexai-1.71.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "vertexai"
                ]
              },
              "id": "88f2dac91ce047aa8827a3a886636742"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "RX9rwYTMUAGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/authentication.json\"\n",
        "os.environ[\"GOOGLE_REGION\"] = \"us-east1\"\n",
        "os.environ[\"GOOGLE_PROJECT_ID\"] = \"emerald-lattice-424023-k3\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "56CaAM3cRFmm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify LLMs to Compare\n"
      ],
      "metadata": {
        "id": "-CH9nr0tYH8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "client = ai.Client()"
      ],
      "metadata": {
        "id": "uWjp8pSAS0FQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "llms = [\n",
        "        \"huggingface:mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        \"openai:gpt-4o-mini\",\n",
        "        \"google:gemini-1.5-flash\"\n",
        "       ]\n",
        "\n",
        "def compare_llm(messages):\n",
        "    execution_times = []\n",
        "    responses = []\n",
        "    for llm in llms:\n",
        "        start_time = time.time()\n",
        "        response = client.chat.completions.create(model=llm, messages=messages)\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        responses.append(response.choices[0].message.content.strip())\n",
        "        execution_times.append(execution_time)\n",
        "        print(f\"{llm} - {execution_time:.2f} seconds: {response.choices[0].message.content.strip()}\")\n",
        "    return responses, execution_times"
      ],
      "metadata": {
        "id": "O6RvWnQUS7iA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Reversal Curse\n"
      ],
      "metadata": {
        "id": "2nH7m3oSTW-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who is Tom Cruise's mother?\"},\n",
        "]\n",
        "\n",
        "responses, execution_times = compare_llm(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaX_LROqTXlb",
        "outputId": "be293426-c979-4d76-d9ca-ebc36257ebec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface:mistralai/Mistral-7B-Instruct-v0.3 - 2.12 seconds: Tom Cruise's mother is Mary Lee Pfeiffer, born on April 24, 1939. She was married to Thomas Cruise Mapother III, Tom's father, from 1970 until their divorce in 1980. Mary Lee passed away on January 19, 2017.\n",
            "openai:gpt-4o-mini - 0.80 seconds: Tom Cruise's mother is Mary Lee South. She was a special education teacher and played a significant role in Tom's upbringing and early life.\n",
            "google:gemini-1.5-flash - 0.43 seconds: Tom Cruise's mother is **Mary Lee Pfeiffer**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def display(llms, execution_times, responses):\n",
        "    data = {\n",
        "        'Provider:Model Name': llms,\n",
        "        'Execution Time': execution_times,\n",
        "        'Model Response ': responses\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.index = df.index + 1\n",
        "    styled_df = df.style.set_table_styles(\n",
        "        [{'selector': 'th', 'props': [('text-align', 'center')]},\n",
        "         {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
        "    ).set_properties(**{'text-align': 'center'})\n",
        "\n",
        "    return styled_df"
      ],
      "metadata": {
        "id": "lndCu4K4YLwn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(llms, execution_times, responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "TcKvbaoeYNmS",
        "outputId": "b3c6019f-0b05-4ec7-f55d-4f2af1e215d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79ddeaa28f70>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_eeccb th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_eeccb td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_eeccb_row0_col0, #T_eeccb_row0_col1, #T_eeccb_row0_col2, #T_eeccb_row1_col0, #T_eeccb_row1_col1, #T_eeccb_row1_col2, #T_eeccb_row2_col0, #T_eeccb_row2_col1, #T_eeccb_row2_col2 {\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_eeccb\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_eeccb_level0_col0\" class=\"col_heading level0 col0\" >Provider:Model Name</th>\n",
              "      <th id=\"T_eeccb_level0_col1\" class=\"col_heading level0 col1\" >Execution Time</th>\n",
              "      <th id=\"T_eeccb_level0_col2\" class=\"col_heading level0 col2\" >Model Response </th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_eeccb_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
              "      <td id=\"T_eeccb_row0_col0\" class=\"data row0 col0\" >huggingface:mistralai/Mistral-7B-Instruct-v0.3</td>\n",
              "      <td id=\"T_eeccb_row0_col1\" class=\"data row0 col1\" >2.123339</td>\n",
              "      <td id=\"T_eeccb_row0_col2\" class=\"data row0 col2\" >Tom Cruise's mother is Mary Lee Pfeiffer, born on April 24, 1939. She was married to Thomas Cruise Mapother III, Tom's father, from 1970 until their divorce in 1980. Mary Lee passed away on January 19, 2017.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_eeccb_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_eeccb_row1_col0\" class=\"data row1 col0\" >openai:gpt-4o-mini</td>\n",
              "      <td id=\"T_eeccb_row1_col1\" class=\"data row1 col1\" >0.801061</td>\n",
              "      <td id=\"T_eeccb_row1_col2\" class=\"data row1 col2\" >Tom Cruise's mother is Mary Lee South. She was a special education teacher and played a significant role in Tom's upbringing and early life.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_eeccb_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
              "      <td id=\"T_eeccb_row2_col0\" class=\"data row2 col0\" >google:gemini-1.5-flash</td>\n",
              "      <td id=\"T_eeccb_row2_col1\" class=\"data row2 col1\" >0.431814</td>\n",
              "      <td id=\"T_eeccb_row2_col2\" class=\"data row2 col2\" >Tom Cruise's mother is **Mary Lee Pfeiffer**.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who is Mary Lee Pfeiffer's son?\"},\n",
        "]\n",
        "\n",
        "responses, execution_times = compare_llm(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VpK0V8nYRcF",
        "outputId": "f9fd4996-58d6-4f47-a3f2-a9299febf901"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface:mistralai/Mistral-7B-Instruct-v0.3 - 1.99 seconds: I don't have specific personal data like family relationships. Mary Lee Pfeiffer is a well-known American journalist and author, but I don't have information about her son. If you're looking for this information, I would suggest checking publicly available resources such as news articles or her official websites for more details.\n",
            "openai:gpt-4o-mini - 0.62 seconds: Mary Lee Pfeiffer is the mother of Tom Cruise, the famous American actor and producer.\n",
            "google:gemini-1.5-flash - 0.95 seconds: There is no publicly available information about Mary Lee Pfeiffer having a son. \n",
            "\n",
            "It's important to note that:\n",
            "\n",
            "* **Privacy:**  Information about someone's family is often considered private. Unless it's publicly known through their own actions or media coverage, it's not appropriate to speculate or share details about their personal lives. \n",
            "* **Multiple People:** There may be many individuals named \"Mary Lee Pfeiffer,\" and without further context, it's impossible to determine who you are referring to.\n",
            "\n",
            "If you have more information about Mary Lee Pfeiffer, you may be able to find more details through online searches or other resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(llms, execution_times, responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "NWKTM3pyYXql",
        "outputId": "87cbf4e0-85eb-403b-cd78-547f6497be7d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79ddd646cfa0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_23f2f th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_23f2f td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_23f2f_row0_col0, #T_23f2f_row0_col1, #T_23f2f_row0_col2, #T_23f2f_row1_col0, #T_23f2f_row1_col1, #T_23f2f_row1_col2, #T_23f2f_row2_col0, #T_23f2f_row2_col1, #T_23f2f_row2_col2 {\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_23f2f\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_23f2f_level0_col0\" class=\"col_heading level0 col0\" >Provider:Model Name</th>\n",
              "      <th id=\"T_23f2f_level0_col1\" class=\"col_heading level0 col1\" >Execution Time</th>\n",
              "      <th id=\"T_23f2f_level0_col2\" class=\"col_heading level0 col2\" >Model Response </th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_23f2f_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
              "      <td id=\"T_23f2f_row0_col0\" class=\"data row0 col0\" >huggingface:mistralai/Mistral-7B-Instruct-v0.3</td>\n",
              "      <td id=\"T_23f2f_row0_col1\" class=\"data row0 col1\" >1.990222</td>\n",
              "      <td id=\"T_23f2f_row0_col2\" class=\"data row0 col2\" >I don't have specific personal data like family relationships. Mary Lee Pfeiffer is a well-known American journalist and author, but I don't have information about her son. If you're looking for this information, I would suggest checking publicly available resources such as news articles or her official websites for more details.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_23f2f_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_23f2f_row1_col0\" class=\"data row1 col0\" >openai:gpt-4o-mini</td>\n",
              "      <td id=\"T_23f2f_row1_col1\" class=\"data row1 col1\" >0.615014</td>\n",
              "      <td id=\"T_23f2f_row1_col2\" class=\"data row1 col2\" >Mary Lee Pfeiffer is the mother of Tom Cruise, the famous American actor and producer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_23f2f_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
              "      <td id=\"T_23f2f_row2_col0\" class=\"data row2 col0\" >google:gemini-1.5-flash</td>\n",
              "      <td id=\"T_23f2f_row2_col1\" class=\"data row2 col1\" >0.952821</td>\n",
              "      <td id=\"T_23f2f_row2_col2\" class=\"data row2 col2\" >There is no publicly available information about Mary Lee Pfeiffer having a son. \n",
              "\n",
              "It's important to note that:\n",
              "\n",
              "* **Privacy:**  Information about someone's family is often considered private. Unless it's publicly known through their own actions or media coverage, it's not appropriate to speculate or share details about their personal lives. \n",
              "* **Multiple People:** There may be many individuals named \"Mary Lee Pfeiffer,\" and without further context, it's impossible to determine who you are referring to.\n",
              "\n",
              "If you have more information about Mary Lee Pfeiffer, you may be able to find more details through online searches or other resources.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How many r's in the word strawberry?\n"
      ],
      "metadata": {
        "id": "2wQ2avmOYasr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How many r's in the word strawberry?\"},\n",
        "]\n",
        "\n",
        "responses, execution_times = compare_llm(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phPCOIVPYcxY",
        "outputId": "2b88b676-0855-449f-bd26-6d5d0ec06c19"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface:mistralai/Mistral-7B-Instruct-v0.3 - 1.42 seconds: There is 1 'r' in the word strawberry.\n",
            "openai:gpt-4o-mini - 0.65 seconds: The word \"strawberry\" contains 2 letter 'r's.\n",
            "google:gemini-1.5-flash - 0.33 seconds: There are **three** \"r\"s in the word \"strawberry\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(llms, execution_times, responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VZS45CmiYfD1",
        "outputId": "7149474e-ef3f-4a81-f4cb-56ea8dbf9a7c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79ddd646d330>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_8667f th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_8667f td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_8667f_row0_col0, #T_8667f_row0_col1, #T_8667f_row0_col2, #T_8667f_row1_col0, #T_8667f_row1_col1, #T_8667f_row1_col2, #T_8667f_row2_col0, #T_8667f_row2_col1, #T_8667f_row2_col2 {\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_8667f\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_8667f_level0_col0\" class=\"col_heading level0 col0\" >Provider:Model Name</th>\n",
              "      <th id=\"T_8667f_level0_col1\" class=\"col_heading level0 col1\" >Execution Time</th>\n",
              "      <th id=\"T_8667f_level0_col2\" class=\"col_heading level0 col2\" >Model Response </th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_8667f_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
              "      <td id=\"T_8667f_row0_col0\" class=\"data row0 col0\" >huggingface:mistralai/Mistral-7B-Instruct-v0.3</td>\n",
              "      <td id=\"T_8667f_row0_col1\" class=\"data row0 col1\" >1.416731</td>\n",
              "      <td id=\"T_8667f_row0_col2\" class=\"data row0 col2\" >There is 1 'r' in the word strawberry.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8667f_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_8667f_row1_col0\" class=\"data row1 col0\" >openai:gpt-4o-mini</td>\n",
              "      <td id=\"T_8667f_row1_col1\" class=\"data row1 col1\" >0.646009</td>\n",
              "      <td id=\"T_8667f_row1_col2\" class=\"data row1 col2\" >The word \"strawberry\" contains 2 letter 'r's.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8667f_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
              "      <td id=\"T_8667f_row2_col0\" class=\"data row2 col0\" >google:gemini-1.5-flash</td>\n",
              "      <td id=\"T_8667f_row2_col1\" class=\"data row2 col1\" >0.329325</td>\n",
              "      <td id=\"T_8667f_row2_col2\" class=\"data row2 col2\" >There are **three** \"r\"s in the word \"strawberry\".</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Which number is bigger?\n"
      ],
      "metadata": {
        "id": "02duR0PjYivg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Which number is bigger, 9.11 or 9.9?\"},\n",
        "]\n",
        "\n",
        "responses, execution_times = compare_llm(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EphR2PE0Yg_V",
        "outputId": "8079083b-7781-4d0c-8747-a61f45b5d85d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface:mistralai/Mistral-7B-Instruct-v0.3 - 1.08 seconds: The number 9.9 is bigger than 9.11. In both numbers, the tens place (9) is the same, but the ones place (9 for 9.9 and 1 for 9.11) determines the difference. Since 9 is greater than 1, 9.9 is a larger decimal number.\n",
            "openai:gpt-4o-mini - 0.54 seconds: 9.9 is bigger than 9.11.\n",
            "google:gemini-1.5-flash - 0.33 seconds: 9.9 is bigger than 9.11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(llms, execution_times, responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "23YKJRC5YnlE",
        "outputId": "a535667c-c2da-416f-8543-cdb41ed11755"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79ddd62e3e20>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_7ba47 th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_7ba47 td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_7ba47_row0_col0, #T_7ba47_row0_col1, #T_7ba47_row0_col2, #T_7ba47_row1_col0, #T_7ba47_row1_col1, #T_7ba47_row1_col2, #T_7ba47_row2_col0, #T_7ba47_row2_col1, #T_7ba47_row2_col2 {\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_7ba47\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_7ba47_level0_col0\" class=\"col_heading level0 col0\" >Provider:Model Name</th>\n",
              "      <th id=\"T_7ba47_level0_col1\" class=\"col_heading level0 col1\" >Execution Time</th>\n",
              "      <th id=\"T_7ba47_level0_col2\" class=\"col_heading level0 col2\" >Model Response </th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_7ba47_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
              "      <td id=\"T_7ba47_row0_col0\" class=\"data row0 col0\" >huggingface:mistralai/Mistral-7B-Instruct-v0.3</td>\n",
              "      <td id=\"T_7ba47_row0_col1\" class=\"data row0 col1\" >1.083907</td>\n",
              "      <td id=\"T_7ba47_row0_col2\" class=\"data row0 col2\" >The number 9.9 is bigger than 9.11. In both numbers, the tens place (9) is the same, but the ones place (9 for 9.9 and 1 for 9.11) determines the difference. Since 9 is greater than 1, 9.9 is a larger decimal number.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7ba47_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_7ba47_row1_col0\" class=\"data row1 col0\" >openai:gpt-4o-mini</td>\n",
              "      <td id=\"T_7ba47_row1_col1\" class=\"data row1 col1\" >0.543441</td>\n",
              "      <td id=\"T_7ba47_row1_col2\" class=\"data row1 col2\" >9.9 is bigger than 9.11.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7ba47_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
              "      <td id=\"T_7ba47_row2_col0\" class=\"data row2 col0\" >google:gemini-1.5-flash</td>\n",
              "      <td id=\"T_7ba47_row2_col1\" class=\"data row2 col1\" >0.325133</td>\n",
              "      <td id=\"T_7ba47_row2_col2\" class=\"data row2 col2\" >9.9 is bigger than 9.11.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Which number is bigger, 9.11 or 9.9? Think step by step.\"},\n",
        "]\n",
        "\n",
        "responses, execution_times = compare_llm(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_V7b2-kYqC8",
        "outputId": "e10b6e11-7b00-4cf8-d795-8a1994f94aa0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface:mistralai/Mistral-7B-Instruct-v0.3 - 1.34 seconds: To compare the numbers, we simply have to look at the digits within the decimal point. Here, we have 9.11 and 9.9. The numbers immediately after the decimal point for both numbers are 1 and 9. As we are comparing them, the one that is larger is the one with the higher number after the decimal point. In this case, that would be 9.9. So, 9.9 is a larger number than 9.\n",
            "openai:gpt-4o-mini - 3.91 seconds: To determine which number is bigger, 9.11 or 9.9, we can compare them step by step:\n",
            "\n",
            "1. **Identify the whole number part**: Both numbers have a whole number part of 9.\n",
            "\n",
            "2. **Compare the decimal parts**:\n",
            "   - 9.11 has a decimal part of 0.11.\n",
            "   - 9.9 can be considered as 9.90 for easier comparison, which has a decimal part of 0.90.\n",
            "\n",
            "3. **Compare the decimal values**:\n",
            "   - 0.11 is less than 0.90.\n",
            "\n",
            "4. **Conclusion**: Since the whole number parts are equal, but the decimal part of 9.9 (0.90) is greater than that of 9.11 (0.11), it follows that 9.9 is the larger number.\n",
            "\n",
            "Thus, **9.9 is bigger than 9.11**.\n",
            "google:gemini-1.5-flash - 0.94 seconds: Here's how to figure out which number is bigger:\n",
            "\n",
            "1. **Focus on the whole number:** Both numbers have the same whole number, which is 9.\n",
            "2. **Compare the decimal parts:**  The decimal part of 9.11 is .11, and the decimal part of 9.9 is .9.\n",
            "3. **Think of money:**  Imagine you have $0.11 (eleven cents) and $0.90 (ninety cents). Which is bigger?  $0.90 is bigger.\n",
            "4. **Conclusion:** Since .9 is bigger than .11, then 9.9 is bigger than 9.11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(llms, execution_times, responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "qb-e0JrQYxhl",
        "outputId": "5fffeefc-3fa1-4e8d-cf8b-9af4b3f7c653"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79ddd62e2350>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_c2738 th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_c2738 td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_c2738_row0_col0, #T_c2738_row0_col1, #T_c2738_row0_col2, #T_c2738_row1_col0, #T_c2738_row1_col1, #T_c2738_row1_col2, #T_c2738_row2_col0, #T_c2738_row2_col1, #T_c2738_row2_col2 {\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_c2738\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c2738_level0_col0\" class=\"col_heading level0 col0\" >Provider:Model Name</th>\n",
              "      <th id=\"T_c2738_level0_col1\" class=\"col_heading level0 col1\" >Execution Time</th>\n",
              "      <th id=\"T_c2738_level0_col2\" class=\"col_heading level0 col2\" >Model Response </th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c2738_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
              "      <td id=\"T_c2738_row0_col0\" class=\"data row0 col0\" >huggingface:mistralai/Mistral-7B-Instruct-v0.3</td>\n",
              "      <td id=\"T_c2738_row0_col1\" class=\"data row0 col1\" >1.340913</td>\n",
              "      <td id=\"T_c2738_row0_col2\" class=\"data row0 col2\" >To compare the numbers, we simply have to look at the digits within the decimal point. Here, we have 9.11 and 9.9. The numbers immediately after the decimal point for both numbers are 1 and 9. As we are comparing them, the one that is larger is the one with the higher number after the decimal point. In this case, that would be 9.9. So, 9.9 is a larger number than 9.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c2738_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_c2738_row1_col0\" class=\"data row1 col0\" >openai:gpt-4o-mini</td>\n",
              "      <td id=\"T_c2738_row1_col1\" class=\"data row1 col1\" >3.911598</td>\n",
              "      <td id=\"T_c2738_row1_col2\" class=\"data row1 col2\" >To determine which number is bigger, 9.11 or 9.9, we can compare them step by step:\n",
              "\n",
              "1. **Identify the whole number part**: Both numbers have a whole number part of 9.\n",
              "\n",
              "2. **Compare the decimal parts**:\n",
              "   - 9.11 has a decimal part of 0.11.\n",
              "   - 9.9 can be considered as 9.90 for easier comparison, which has a decimal part of 0.90.\n",
              "\n",
              "3. **Compare the decimal values**:\n",
              "   - 0.11 is less than 0.90.\n",
              "\n",
              "4. **Conclusion**: Since the whole number parts are equal, but the decimal part of 9.9 (0.90) is greater than that of 9.11 (0.11), it follows that 9.9 is the larger number.\n",
              "\n",
              "Thus, **9.9 is bigger than 9.11**.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c2738_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
              "      <td id=\"T_c2738_row2_col0\" class=\"data row2 col0\" >google:gemini-1.5-flash</td>\n",
              "      <td id=\"T_c2738_row2_col1\" class=\"data row2 col1\" >0.941795</td>\n",
              "      <td id=\"T_c2738_row2_col2\" class=\"data row2 col2\" >Here's how to figure out which number is bigger:\n",
              "\n",
              "1. **Focus on the whole number:** Both numbers have the same whole number, which is 9.\n",
              "2. **Compare the decimal parts:**  The decimal part of 9.11 is .11, and the decimal part of 9.9 is .9.\n",
              "3. **Think of money:**  Imagine you have $0.11 (eleven cents) and $0.90 (ninety cents). Which is bigger?  $0.90 is bigger.\n",
              "4. **Conclusion:** Since .9 is bigger than .11, then 9.9 is bigger than 9.11.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "1. Not all LLMs are created equal - not even all Llama 3 (or 3.1) are created equal (by different providers).\n",
        "2. Ask LLM to think step by step may help improve its reasoning.\n",
        "3. The way tokenization works in LLM could lead to a lot of weirdness in LLM (see AK's awesome [video](https://www.youtube.com/watch?v=zduSFxRajkE) for a deep dive).\n",
        "4. A more comprehensive benchmark would be desired, but a quick LLM comparison like shown here can be the first step."
      ],
      "metadata": {
        "id": "d7Zlf3w2Y07l"
      }
    }
  ]
}